<!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Scrapy 爬虫开发 · BooTell</title><meta name="description" content="Scrapy 爬虫开发 - Brooks Bao"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://bootell.net/atom.xml" title="BooTell"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="BooTell" type="application/atom+xml">
</head><body><div class="wrap"><header><a class="logo-link" href="/"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a class="nav-list-link" href="/" target="_self">BLOG</a></li><li class="nav-list-item"><a class="nav-list-link" href="/archives/" target="_self">ARCHIVE</a></li><li class="nav-list-item"><a class="nav-list-link" href="https://github.com/bootell" target="_blank">GITHUB</a></li><li class="nav-list-item"><a class="nav-list-link" href="/atom.xml" target="_self">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Scrapy 爬虫开发</h1><div class="post-info">2019-08-06<a class="post-tag" href="/tags/%E7%88%AC%E8%99%AB/">#爬虫</a></div><div class="post-content"><h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h3><p>Scrapy 支持 Python2.7 及 3.4+，安装步骤按照<a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/intro/install.html">官方文档</a>进行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">pip3 install Scrapy</span><br><span class="line"><span class="comment"># 创建项目</span></span><br><span class="line">scrapy startproject crawler</span><br></pre></td></tr></table></figure>


<p>安装完毕后，目录结构如下所示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">crawler&#x2F;</span><br><span class="line">    scrapy.cfg            # scrapyd-client 项目部署时使用</span><br><span class="line">    crawler&#x2F;</span><br><span class="line">        __init__.py</span><br><span class="line">        items.py          # 爬取结果结构化</span><br><span class="line">        middlewares.py    # 项目中间件</span><br><span class="line">        pipelines.py      # 项目管道</span><br><span class="line">        settings.py       # 项目设置</span><br><span class="line">        spiders&#x2F;          # 爬虫所在目录</span><br><span class="line">            __init__.py</span><br></pre></td></tr></table></figure>


<p>Scrapy 执行时的流程大致是</p>
<p><img src="/images/scrapy_flow.png" alt="Scrapy 流程"></p>
<ol>
<li>Engine 从 Spider 获取 <code>start_urls</code></li>
<li>Engine 将 <code>start_urls</code> 发送到 Scheduler，并请求下一个爬取的 Request</li>
<li>Scheduler 将下一个要爬取 Request 返回给 Engine</li>
<li>Engine 将收到的 Request 执行所有 Middleware 的 <code>process_request()</code> 后，发送到 Downloader</li>
<li>Downloader 下载内容后，执行所有 Middleware 的 <code>process_response()</code> 后，将结果返回给 Engine</li>
<li>Engine 将内容发送给 Spider 做数据处理，之前执行 Middleware 的 <code>process_spider_input()</code></li>
<li>Spider 处理后，将结果通过 Middleware 的 <code>process_spider_output()</code> 后，返回给 Engine</li>
<li>Engine 将处理后的数据发送给 Pipline 进行操作，并将处理过的 Request 发送给 Scheduler，请求下一个 Request</li>
</ol>
<p><code>settings.py</code> 配置文件中，一般需要修改的配置如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 是否遵循 robots 协议</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"><span class="comment"># 并发请求数</span></span><br><span class="line">CONCURRENT_REQUESTS = <span class="number">16</span></span><br><span class="line"><span class="comment"># 下载间隔，实际范围在 0.5*DOWNLOAD_DELAY 到 1.5*DOWNLOAD_DELAY 之间</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">1</span></span><br><span class="line"><span class="comment"># 域名/IP 并发请求限制</span></span><br><span class="line">CONCURRENT_REQUESTS_PER_DOMAIN = <span class="number">16</span></span><br><span class="line">CONCURRENT_REQUESTS_PER_IP = <span class="number">16</span></span><br><span class="line"><span class="comment"># 启用 Cookies</span></span><br><span class="line">COOKIES_ENABLED = <span class="literal">False</span></span><br><span class="line"><span class="comment"># 下载中间件，后面数字表示优先级</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;&#125;</span><br><span class="line"><span class="comment"># 管道</span></span><br><span class="line">ITEM_PIPELINES = &#123;&#125;</span><br></pre></td></tr></table></figure>



<h3 id="Spider"><a href="#Spider" class="headerlink" title="Spider"></a>Spider</h3><p>框架事先定义了几个通用的爬虫</p>
<ul>
<li><code>scrapy.spiders.Spider</code> 最基本的爬虫</li>
<li><code>scrapy.spiders.CrawlSpider</code> 最常用爬虫，能根据规则对全站网站进行爬取</li>
<li><code>scrapy.spiders.XMLFeedSpider</code></li>
<li><code>scrapy.spiders.CSVFeedSpider</code></li>
<li><code>scrapy.spiders.SitemapSpider</code></li>
</ul>
<p><code>CrawlSpider</code> 继承了 <code>Spider</code> 外，提供了额外的属性和方法：</p>
<ul>
<li><code>rules</code>：是 <code>Rule</code> 对象的列表，定义了爬取网站的规则。它对不同的连接所需要执行的动作进行了定义。</li>
<li><code>parse_start_url()</code>：<code>start_url</code> 的请求返回时，该方法会被调用</li>
</ul>
<p><code>Rule</code> 对象主要作用为过滤有效链接，指定链接处理方法，并确定是否继续跟进</p>
<ul>
<li><code>link_extractor</code> 从爬取的页面中提取指定格式的链接，生成新的 Request 请求，规则有 <code>allow</code>，<code>deny</code> 等，详细见<a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/link-extractors.html#topics-link-extractors">官方文档</a></li>
<li><code>callback</code> 对格式匹配的页面执行对应的处理方法</li>
<li><code>cb_kwargs</code> 回掉函数的参数</li>
<li><code>follow</code> 是否对页面中的连接继续跟进，当 <code>callback</code> 为 <code>None</code> 时默认为 <code>True</code>，其他默认为 <code>False</code></li>
<li><code>process_links</code> 过滤提取的链接</li>
<li><code>process_request</code> 对指定链接 Request 请求进行处理</li>
</ul>
<p>完整示例 <code>crawlwer/spiders/ithome.py</code> 如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> CrawlerItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ithome</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">    <span class="comment"># 爬虫名臣</span></span><br><span class="line">    name = <span class="string">&quot;ithome&quot;</span></span><br><span class="line">    <span class="comment"># 允许爬取的域名</span></span><br><span class="line">    allowed_domains = [</span><br><span class="line">        <span class="string">&#x27;ithome.com&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># 初始链接</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.ithome.com&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 爬取的规则</span></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">&#x27;[0-9]+\.htm&#x27;</span>)), callback=<span class="string">&#x27;parse_article&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">&#x27;.*\.htm&#x27;</span>))),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_article</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        self.logger.info(<span class="string">&#x27;Parsing url: %s&#x27;</span>, response.url)</span><br><span class="line">        item = CrawlerItem()</span><br><span class="line">        item[<span class="string">&#x27;url&#x27;</span>] = response.url</span><br><span class="line">        item[<span class="string">&#x27;title&#x27;</span>] = response.css(<span class="string">&#x27;.post_title h1::text&#x27;</span>).get()</span><br><span class="line">        content = response.css(<span class="string">&#x27;.post_content p ::text&#x27;</span>).getall()</span><br><span class="line">        item[<span class="string">&#x27;content&#x27;</span>] = <span class="string">&#x27;&#x27;</span>.join(content)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>



<h3 id="Items"><a href="#Items" class="headerlink" title="Items"></a>Items</h3><p>用来定义结构化的结果<br>完整示例 <code>crawler/items.py</code> 如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrawlerItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br><span class="line">    comments = scrapy.Field()</span><br></pre></td></tr></table></figure>



<h3 id="Middlewares"><a href="#Middlewares" class="headerlink" title="Middlewares"></a>Middlewares</h3><p>middleware 主要用来在下载，处理爬虫等前后，进行相关操作。<br>本次主要用来处理反爬虫。最基本反爬虫一般是通过浏览器的 UA，客户端的 IP，以及动态加载的 JS 来实现。于是针对以上措施，分别进行处理。</p>
<ul>
<li>请求 UA 随机中间件<br>需要安装 <code>pip3 install fake-useragent</code>，它从 <code>useragentstring.com</code> 和 <code>w3schools.com</code> 获取真实的浏览器 useragent，并在本地进行缓存<br>最早时从网上找了一些 UA，在本地做了一个随机获取，结果网上的 UA 已被翻爬虫过滤了，不能绕过反爬机制<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserAgentMiddleware</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.ua = UserAgent()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        request.headers.setdefault(<span class="string">&#x27;User-Agent&#x27;</span>, self.ua.random)</span><br></pre></td></tr></table></figure></li>
</ul>
<ul>
<li>随机代理中间件<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyMiddleware</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, from_url, proxy_pool_key</span>):</span></span><br><span class="line">        self.redis = redis.from_url(from_url)</span><br><span class="line">        self.proxy_pool_key = proxy_pool_key</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            from_url=crawler.settings.get(<span class="string">&#x27;REDIS_URL&#x27;</span>),</span><br><span class="line">            proxy_pool_key=crawler.settings.get(<span class="string">&#x27;PROXY_POOL_KEY&#x27;</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        proxy = self.redis.srandmember(self.proxy_pool_key)</span><br><span class="line">        <span class="keyword">if</span> proxy:</span><br><span class="line">            proxy = proxy.decode()</span><br><span class="line">            spider.logger.info(<span class="string">&#x27;Using proxy: %s&#x27;</span>, proxy)</span><br><span class="line">            request.meta[<span class="string">&#x27;proxy&#x27;</span>] = proxy</span><br></pre></td></tr></table></figure></li>
</ul>
<blockquote>
<h4 id="代理配置"><a href="#代理配置" class="headerlink" title="代理配置"></a>代理配置</h4><p>服务器有多个IP，可以使用 <a target="_blank" rel="noopener" href="https://wiki.squid-cache.org/">squid</a> 创建 http 代理服务器，通过设置代理不同端口使用不同的 IP 地址。</p>
<p>安装直接通过 <code>apt-get install squid</code> ，安装完成后修改配置文件 <code>/etc/squid/squid.conf</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 定义权限规则列表</span><br><span class="line">acl port3128 localport 3128</span><br><span class="line">acl port3129 localport 3129</span><br><span class="line">acl port3130 localport 3130</span><br><span class="line">acl port3131 localport 3131</span><br><span class="line">acl port3132 localport 3132</span><br><span class="line">acl port3133 localport 3133</span><br><span class="line"></span><br><span class="line"># 定义访问控制，允许接入的地址</span><br><span class="line">http_access allow all</span><br><span class="line"></span><br><span class="line"># 定义监听的端口</span><br><span class="line">http_port 3128</span><br><span class="line">http_port 3129</span><br><span class="line">http_port 3130</span><br><span class="line">http_port 3131</span><br><span class="line">http_port 3132</span><br><span class="line">http_port 3133</span><br><span class="line"></span><br><span class="line"># 定义转发地址</span><br><span class="line">tcp_outgoing_address 172.16.0.106 port3128</span><br><span class="line">tcp_outgoing_address 172.16.0.107 port3129</span><br><span class="line">tcp_outgoing_address 172.16.0.108 port3130</span><br><span class="line">tcp_outgoing_address 172.16.0.109 port3131</span><br><span class="line">tcp_outgoing_address 172.16.0.110 port3132</span><br><span class="line">tcp_outgoing_address 172.16.0.248 port3133</span><br></pre></td></tr></table></figure>
</blockquote>
<ul>
<li>CloudFlare 反爬虫，起主要反爬方法是通过 JS 生成本地 Cookie。<br>可以通过 <a target="_blank" rel="noopener" href="https://github.com/clemfromspace/scrapy-cloudflare-middleware">scrapy_cloudflare_middleware</a> 进行处理，直接安装 <code>pip3 install scrapy_cloudflare_middleware</code></li>
</ul>
<p>启动的 Middlewares 需要写入 <code>settings.py</code> 配置文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CloudFlare 反爬需要开启 cookie</span></span><br><span class="line">COOKIES_ENABLED = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="comment"># 禁用 Scrapy 自带的 UA Middleware</span></span><br><span class="line">    <span class="string">&#x27;scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware&#x27;</span>: <span class="literal">None</span>, </span><br><span class="line">    <span class="string">&#x27;crawler.middlewares.UserAgentMiddleware&#x27;</span>: <span class="number">400</span>,</span><br><span class="line">    <span class="string">&#x27;crawler.middlewares.ProxyMiddleware&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy_cloudflare_middleware.middlewares.CloudFlareMiddleware&#x27;</span>: <span class="number">560</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="Piplines"><a href="#Piplines" class="headerlink" title="Piplines"></a>Piplines</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    collection_name = <span class="string">&#x27;scrapy_items&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, mongo_uri, mongo_db</span>):</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">&#x27;MONGO_URI&#x27;</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">&#x27;MONGO_DATABASE&#x27;</span>, <span class="string">&#x27;items&#x27;</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        self.db[self.collection_name].insert_one(<span class="built_in">dict</span>(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>在 <code>settings.py</code> 配置中增加</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;crawler.pipelines.MongoPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">MONGO_URI=<span class="string">&#x27;mongodb://127.0.0.1:27017&#x27;</span></span><br><span class="line">MONGO_DATABASE = <span class="string">&#x27;items&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="Distributed-crawling"><a href="#Distributed-crawling" class="headerlink" title="Distributed crawling"></a>Distributed crawling</h3><p>Scrapy Scheduler 和 Duplication Filter 本身使用了本地文件来来存储，不能进行水平的扩展。可以使用 <a target="_blank" rel="noopener" href="https://github.com/rmax/scrapy-redis">Scrapy-Redis</a> 来存放这些数据，使爬虫能够方便扩展，可以分布式部署。</p>
<p>安装通过 <code>pip3 install Scrapy-Redis</code>，安装后修改 settings 配置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SCHEDULER = <span class="string">&quot;scrapy_redis.scheduler.Scheduler&quot;</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">&quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span></span><br><span class="line">SCHEDULER_PERSIST = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">REDIS_URL=<span class="string">&#x27;redis://:password@127.0.0.1:6379/0&#x27;</span></span><br></pre></td></tr></table></figure>

<p>修改 Spider 继承 <code>scrapy_redis.spiders.RedisCrawlSpider</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy_redis.spiders <span class="keyword">import</span> RedisCrawlSpider</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ithome</span>(<span class="params">RedisCrawlSpider</span>):</span></span><br><span class="line">    <span class="comment"># start_urls = [&#x27;https://www.ithome.com&#x27;]</span></span><br></pre></td></tr></table></figure>

<p>由于任务是从 redis 中读取，所以 <code>start_urls</code> 需要直接存入 redis <code>redis-cli lpush ithome:start_urls https://www.ithome.com</code></p>
<h3 id="Deploy"><a href="#Deploy" class="headerlink" title="Deploy"></a>Deploy</h3><p>只运行单个爬虫时，直接使用 <code>scrapy crawl spider-name</code> 命令来运行，按 <code>Ctrl+C</code> 来停止。</p>
<p>部署到服务器执行时，需要执行多个爬虫，可以通过 <code>scrapyd</code> 服务运行、监控。<br>首先安装 <code>pip3 install scrapyd </code>，然后在项目目录创建配置文件 <code>scrapyd.conf</code> 如下</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[scrapyd]</span></span><br><span class="line"><span class="attr">eggs_dir</span>    = eggs</span><br><span class="line"><span class="attr">logs_dir</span>    = logs</span><br><span class="line">items_dir   =</span><br><span class="line"><span class="comment"># 日志最大保存数量</span></span><br><span class="line"><span class="attr">jobs_to_keep</span> = <span class="number">5</span></span><br><span class="line"><span class="attr">dbs_dir</span>     = dbs</span><br><span class="line"><span class="comment"># 同时执行的最大爬虫数量，设为0时，最大执行数量为 max_proc_per_cpu * cpu 核心数</span></span><br><span class="line"><span class="attr">max_proc</span>    = <span class="number">0</span></span><br><span class="line"><span class="comment"># 每个 CPU 最大同时执行爬虫数量</span></span><br><span class="line"><span class="attr">max_proc_per_cpu</span> = <span class="number">4</span></span><br><span class="line"><span class="comment"># 爬虫历史最大保存数量</span></span><br><span class="line"><span class="attr">finished_to_keep</span> = <span class="number">100</span></span><br><span class="line"><span class="attr">poll_interval</span> = <span class="number">5.0</span></span><br><span class="line"><span class="attr">bind_address</span> = <span class="number">127.0</span>.<span class="number">0.1</span></span><br><span class="line"><span class="attr">http_port</span>   = <span class="number">6800</span></span><br><span class="line"><span class="attr">debug</span>       = <span class="literal">off</span></span><br><span class="line"><span class="attr">runner</span>      = scrapyd.runner</span><br><span class="line"><span class="attr">application</span> = scrapyd.app.application</span><br><span class="line"><span class="attr">launcher</span>    = scrapyd.launcher.Launcher</span><br><span class="line"><span class="attr">webroot</span>     = scrapyd.website.Root</span><br><span class="line"><span class="section">[services]</span></span><br><span class="line"><span class="attr">schedule.json</span>     = scrapyd.webservice.Schedule</span><br><span class="line"><span class="attr">cancel.json</span>       = scrapyd.webservice.Cancel</span><br><span class="line"><span class="attr">addversion.json</span>   = scrapyd.webservice.AddVersion</span><br><span class="line"><span class="attr">listprojects.json</span> = scrapyd.webservice.ListProjects</span><br><span class="line"><span class="attr">listversions.json</span> = scrapyd.webservice.ListVersions</span><br><span class="line"><span class="attr">listspiders.json</span>  = scrapyd.webservice.ListSpiders</span><br><span class="line"><span class="attr">delproject.json</span>   = scrapyd.webservice.DeleteProject</span><br><span class="line"><span class="attr">delversion.json</span>   = scrapyd.webservice.DeleteVersion</span><br><span class="line"><span class="attr">listjobs.json</span>     = scrapyd.webservice.ListJobs</span><br><span class="line"><span class="attr">daemonstatus.json</span> = scrapyd.webservice.DaemonStatus</span><br></pre></td></tr></table></figure>

<p>scrapyd 提供了 API 接口用来控制和监控爬虫</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出可运行的所有爬虫</span></span><br><span class="line">curl http://localhost:6800/listspiders.json\?project\=default</span><br><span class="line"><span class="comment"># 执行爬虫</span></span><br><span class="line">curl http://localhost:6800/schedule.json -d project=default -d spider=spider-name </span><br><span class="line"><span class="comment"># 列出所有任务</span></span><br><span class="line">curl http://localhost:6800/listjobs.json?project=default</span><br><span class="line"><span class="comment"># 取消爬虫</span></span><br><span class="line">curl http://localhost:6800/cancel.json -d project=default -d job=&#123;job-id&#125;</span><br></pre></td></tr></table></figure>

<p>另外 scrapyd 还提供了一个 web 界面方便查看，由于 scrapyd 本身没有提供授权机制，可以使用 nginx 反向代理并设置 Basic Auth。创建 nginx 配置文件 <code>/etc/nginx/sites-enabled/scrapyd</code> </p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">6801</span>;</span><br><span class="line">    <span class="attribute">location</span> / &#123;</span><br><span class="line">        <span class="attribute">proxy_pass</span> http://127.0.0.1:6800/;</span><br><span class="line">        <span class="attribute">auth_basic</span> <span class="string">&quot;Restricted&quot;</span>;</span><br><span class="line">        <span class="attribute">auth_basic_user_file</span> /etc/nginx/conf.d/.htpasswd;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>生成密码文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">htpasswd -c /etc/nginx/conf.d/.htpasswd username password</span><br></pre></td></tr></table></figure>



<h3 id="Speed-optimization"><a href="#Speed-optimization" class="headerlink" title="Speed optimization"></a>Speed optimization</h3><p>按照默认配置部署到服务器之后，发现服务器负载非常低，爬取速度也很慢。可以简单的修改配置加快爬虫速度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CONCURRENT_REQUESTS = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">CONCURRENT_REQUESTS_PER_DOMAIN = <span class="number">100</span></span><br><span class="line">CONCURRENT_REQUESTS_PER_IP = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">REACTOR_THREADPOOL_MAXSIZE = <span class="number">20</span></span><br></pre></td></tr></table></figure>

<span id="more"></span></div></article></div></main><footer><div class="paginator"><a class="prev" href="/2019/10/03/Build-Hexo-Using-Github-Actions/">PREV</a><a class="next" href="/2019/02/28/LEDE-K3-Build-with-GitLab-CI/">NEXT</a></div><div class="copyright"><p>© 2016 - 2021 <a href="https://bootell.net">Brooks Bao</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-111290368-1",'auto');ga('send','pageview');</script></body></html>